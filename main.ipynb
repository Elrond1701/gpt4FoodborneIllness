{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1\n",
    "\n",
    "In-Context Learning\n",
    "\n",
    "## TRC\n",
    "\n",
    "不同的in-context length。\n",
    "\n",
    "confusion matrix\n",
    "\n",
    "## EMD\n",
    "\n",
    "不同的in-context length。\n",
    "\n",
    "结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2\n",
    "\n",
    "GPT embedding & last layer classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scripts.TRC import TRC\n",
    "\n",
    "model_names = [\"Adaboost\", \"DecisionTree\", \"GaussianProcess\", \"GradientBoosting\", \"KNN\", \n",
    "               \"LDA\", \"LogisticRegression\", \"NaiveBayes\", \"RandomForest\", \"SVM\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost\n",
      "[[238  37]\n",
      " [ 43  94]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.87      0.86       275\n",
      "           1       0.72      0.69      0.70       137\n",
      "\n",
      "    accuracy                           0.81       412\n",
      "   macro avg       0.78      0.78      0.78       412\n",
      "weighted avg       0.80      0.81      0.80       412\n",
      "\n",
      "DecisionTree\n",
      "[[216  59]\n",
      " [ 61  76]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.79      0.78       275\n",
      "           1       0.56      0.55      0.56       137\n",
      "\n",
      "    accuracy                           0.71       412\n",
      "   macro avg       0.67      0.67      0.67       412\n",
      "weighted avg       0.71      0.71      0.71       412\n",
      "\n",
      "GaussianProcess\n",
      "[[253  22]\n",
      " [ 33 104]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90       275\n",
      "           1       0.83      0.76      0.79       137\n",
      "\n",
      "    accuracy                           0.87       412\n",
      "   macro avg       0.86      0.84      0.85       412\n",
      "weighted avg       0.86      0.87      0.87       412\n",
      "\n",
      "GradientBoosting\n",
      "[[254  21]\n",
      " [ 38  99]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.92      0.90       275\n",
      "           1       0.82      0.72      0.77       137\n",
      "\n",
      "    accuracy                           0.86       412\n",
      "   macro avg       0.85      0.82      0.83       412\n",
      "weighted avg       0.85      0.86      0.85       412\n",
      "\n",
      "KNN\n",
      "[[195  80]\n",
      " [ 20 117]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.71      0.80       275\n",
      "           1       0.59      0.85      0.70       137\n",
      "\n",
      "    accuracy                           0.76       412\n",
      "   macro avg       0.75      0.78      0.75       412\n",
      "weighted avg       0.80      0.76      0.76       412\n",
      "\n",
      "LDA\n",
      "[[216  59]\n",
      " [ 37 100]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.82       275\n",
      "           1       0.63      0.73      0.68       137\n",
      "\n",
      "    accuracy                           0.77       412\n",
      "   macro avg       0.74      0.76      0.75       412\n",
      "weighted avg       0.78      0.77      0.77       412\n",
      "\n",
      "LogisticRegression\n",
      "[[246  29]\n",
      " [ 32 105]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.89       275\n",
      "           1       0.78      0.77      0.77       137\n",
      "\n",
      "    accuracy                           0.85       412\n",
      "   macro avg       0.83      0.83      0.83       412\n",
      "weighted avg       0.85      0.85      0.85       412\n",
      "\n",
      "NaiveBayes\n",
      "[[199  76]\n",
      " [ 11 126]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.72      0.82       275\n",
      "           1       0.62      0.92      0.74       137\n",
      "\n",
      "    accuracy                           0.79       412\n",
      "   macro avg       0.79      0.82      0.78       412\n",
      "weighted avg       0.84      0.79      0.79       412\n",
      "\n",
      "RandomForest\n",
      "[[260  15]\n",
      " [ 54  83]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.95      0.88       275\n",
      "           1       0.85      0.61      0.71       137\n",
      "\n",
      "    accuracy                           0.83       412\n",
      "   macro avg       0.84      0.78      0.79       412\n",
      "weighted avg       0.83      0.83      0.82       412\n",
      "\n",
      "SVM\n",
      "[[243  32]\n",
      " [ 27 110]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89       275\n",
      "           1       0.77      0.80      0.79       137\n",
      "\n",
      "    accuracy                           0.86       412\n",
      "   macro avg       0.84      0.84      0.84       412\n",
      "weighted avg       0.86      0.86      0.86       412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_file = \"./data/English/LREC_expert_label/train.csv\"\n",
    "test_file = \"./data/English/LREC_expert_label/test.csv\"\n",
    "train_dat = pd.read_csv(train_file)\n",
    "test_dat = pd.read_csv(test_file)\n",
    "\n",
    "for model_name in model_names:\n",
    "    confusion_matrix, classification_report = TRC(model_name=model_name, train_dat=train_dat, test_dat=test_dat)\n",
    "    print(model_name)\n",
    "    print(confusion_matrix)\n",
    "    print(classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BSC Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost\n",
      "[[181  94]\n",
      " [ 18 119]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.66      0.76       275\n",
      "           1       0.56      0.87      0.68       137\n",
      "\n",
      "    accuracy                           0.73       412\n",
      "   macro avg       0.73      0.76      0.72       412\n",
      "weighted avg       0.79      0.73      0.74       412\n",
      "\n",
      "DecisionTree\n",
      "[[179  96]\n",
      " [ 39  98]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.65      0.73       275\n",
      "           1       0.51      0.72      0.59       137\n",
      "\n",
      "    accuracy                           0.67       412\n",
      "   macro avg       0.66      0.68      0.66       412\n",
      "weighted avg       0.72      0.67      0.68       412\n",
      "\n",
      "GaussianProcess\n",
      "[[192  83]\n",
      " [  5 132]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.70      0.81       275\n",
      "           1       0.61      0.96      0.75       137\n",
      "\n",
      "    accuracy                           0.79       412\n",
      "   macro avg       0.79      0.83      0.78       412\n",
      "weighted avg       0.85      0.79      0.79       412\n",
      "\n",
      "GradientBoosting\n",
      "[[192  83]\n",
      " [  9 128]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.70      0.81       275\n",
      "           1       0.61      0.93      0.74       137\n",
      "\n",
      "    accuracy                           0.78       412\n",
      "   macro avg       0.78      0.82      0.77       412\n",
      "weighted avg       0.84      0.78      0.78       412\n",
      "\n",
      "KNN\n",
      "[[149 126]\n",
      " [  3 134]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.54      0.70       275\n",
      "           1       0.52      0.98      0.68       137\n",
      "\n",
      "    accuracy                           0.69       412\n",
      "   macro avg       0.75      0.76      0.69       412\n",
      "weighted avg       0.83      0.69      0.69       412\n",
      "\n",
      "LDA\n",
      "[[173 102]\n",
      " [ 16 121]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.63      0.75       275\n",
      "           1       0.54      0.88      0.67       137\n",
      "\n",
      "    accuracy                           0.71       412\n",
      "   macro avg       0.73      0.76      0.71       412\n",
      "weighted avg       0.79      0.71      0.72       412\n",
      "\n",
      "LogisticRegression\n",
      "[[191  84]\n",
      " [  4 133]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.69      0.81       275\n",
      "           1       0.61      0.97      0.75       137\n",
      "\n",
      "    accuracy                           0.79       412\n",
      "   macro avg       0.80      0.83      0.78       412\n",
      "weighted avg       0.86      0.79      0.79       412\n",
      "\n",
      "NaiveBayes\n",
      "[[170 105]\n",
      " [  6 131]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.62      0.75       275\n",
      "           1       0.56      0.96      0.70       137\n",
      "\n",
      "    accuracy                           0.73       412\n",
      "   macro avg       0.76      0.79      0.73       412\n",
      "weighted avg       0.83      0.73      0.74       412\n",
      "\n",
      "RandomForest\n",
      "[[197  78]\n",
      " [  9 128]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.72      0.82       275\n",
      "           1       0.62      0.93      0.75       137\n",
      "\n",
      "    accuracy                           0.79       412\n",
      "   macro avg       0.79      0.83      0.78       412\n",
      "weighted avg       0.84      0.79      0.79       412\n",
      "\n",
      "SVM\n",
      "[[199  76]\n",
      " [  3 134]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.72      0.83       275\n",
      "           1       0.64      0.98      0.77       137\n",
      "\n",
      "    accuracy                           0.81       412\n",
      "   macro avg       0.81      0.85      0.80       412\n",
      "weighted avg       0.87      0.81      0.81       412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_file = \"./data/English/LREC_BSC/train.csv\"\n",
    "test_file = \"./data/English/LREC_BSC/test.csv\"\n",
    "train_dat = pd.read_csv(train_file)\n",
    "test_dat = pd.read_csv(test_file)\n",
    "\n",
    "for model_name in model_names:\n",
    "    confusion_matrix, classification_report = TRC(model_name=model_name, train_dat=train_dat, test_dat=test_dat)\n",
    "    print(model_name)\n",
    "    print(confusion_matrix)\n",
    "    print(classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mv Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost\n",
      "[[193  82]\n",
      " [ 15 122]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.70      0.80       275\n",
      "           1       0.60      0.89      0.72       137\n",
      "\n",
      "    accuracy                           0.76       412\n",
      "   macro avg       0.76      0.80      0.76       412\n",
      "weighted avg       0.82      0.76      0.77       412\n",
      "\n",
      "DecisionTree\n",
      "[[168 107]\n",
      " [ 31 106]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.61      0.71       275\n",
      "           1       0.50      0.77      0.61       137\n",
      "\n",
      "    accuracy                           0.67       412\n",
      "   macro avg       0.67      0.69      0.66       412\n",
      "weighted avg       0.73      0.67      0.67       412\n",
      "\n",
      "GaussianProcess\n",
      "[[192  83]\n",
      " [  5 132]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.70      0.81       275\n",
      "           1       0.61      0.96      0.75       137\n",
      "\n",
      "    accuracy                           0.79       412\n",
      "   macro avg       0.79      0.83      0.78       412\n",
      "weighted avg       0.85      0.79      0.79       412\n",
      "\n",
      "GradientBoosting\n",
      "[[193  82]\n",
      " [ 10 127]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.70      0.81       275\n",
      "           1       0.61      0.93      0.73       137\n",
      "\n",
      "    accuracy                           0.78       412\n",
      "   macro avg       0.78      0.81      0.77       412\n",
      "weighted avg       0.84      0.78      0.78       412\n",
      "\n",
      "KNN\n",
      "[[150 125]\n",
      " [  3 134]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.55      0.70       275\n",
      "           1       0.52      0.98      0.68       137\n",
      "\n",
      "    accuracy                           0.69       412\n",
      "   macro avg       0.75      0.76      0.69       412\n",
      "weighted avg       0.83      0.69      0.69       412\n",
      "\n",
      "LDA\n",
      "[[177  98]\n",
      " [ 16 121]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.64      0.76       275\n",
      "           1       0.55      0.88      0.68       137\n",
      "\n",
      "    accuracy                           0.72       412\n",
      "   macro avg       0.73      0.76      0.72       412\n",
      "weighted avg       0.80      0.72      0.73       412\n",
      "\n",
      "LogisticRegression\n",
      "[[191  84]\n",
      " [  4 133]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.69      0.81       275\n",
      "           1       0.61      0.97      0.75       137\n",
      "\n",
      "    accuracy                           0.79       412\n",
      "   macro avg       0.80      0.83      0.78       412\n",
      "weighted avg       0.86      0.79      0.79       412\n",
      "\n",
      "NaiveBayes\n",
      "[[170 105]\n",
      " [  6 131]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.62      0.75       275\n",
      "           1       0.56      0.96      0.70       137\n",
      "\n",
      "    accuracy                           0.73       412\n",
      "   macro avg       0.76      0.79      0.73       412\n",
      "weighted avg       0.83      0.73      0.74       412\n",
      "\n",
      "RandomForest\n",
      "[[194  81]\n",
      " [ 11 126]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.71      0.81       275\n",
      "           1       0.61      0.92      0.73       137\n",
      "\n",
      "    accuracy                           0.78       412\n",
      "   macro avg       0.78      0.81      0.77       412\n",
      "weighted avg       0.83      0.78      0.78       412\n",
      "\n",
      "SVM\n",
      "[[199  76]\n",
      " [  3 134]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.72      0.83       275\n",
      "           1       0.64      0.98      0.77       137\n",
      "\n",
      "    accuracy                           0.81       412\n",
      "   macro avg       0.81      0.85      0.80       412\n",
      "weighted avg       0.87      0.81      0.81       412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_file = \"./data/English/LREC_mv/train.csv\"\n",
    "test_file = \"./data/English/LREC_mv/test.csv\"\n",
    "train_dat = pd.read_csv(train_file)\n",
    "test_dat = pd.read_csv(test_file)\n",
    "\n",
    "for model_name in model_names:\n",
    "    confusion_matrix, classification_report = TRC(model_name=model_name, train_dat=train_dat, test_dat=test_dat)\n",
    "    print(model_name)\n",
    "    print(confusion_matrix)\n",
    "    print(classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"./data/English/LREC_mv/train.csv\"\n",
    "test_file = \"./data/English/LREC_mv/test.csv\"\n",
    "train_dat = pd.read_csv(train_file)\n",
    "test_dat = pd.read_csv(test_file)\n",
    "\n",
    "for model_name in model_names:\n",
    "    confusion_matrix, classification_report = TRC(model_name=model_name, train_dat=train_dat, test_dat=test_dat)\n",
    "    print(model_name)\n",
    "    print(confusion_matrix)\n",
    "    print(classification_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
